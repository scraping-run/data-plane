# 4-Node High Availability Kubernetes Cluster on Oracle Cloud (Multi-Account)

ì´ ë¬¸ì„œëŠ” **ì—¬ëŸ¬ Oracle Cloud ê³„ì •ì— ë¶„ì‚°ëœ ARM64 ì¸ìŠ¤í„´ìŠ¤ 4ëŒ€**ë¥¼ **Tailscale VPNìœ¼ë¡œ ì—°ê²°**í•˜ì—¬ ê³ ê°€ìš©ì„± Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì¶•í•˜ê³  **Data Plane** ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ¯ í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Multi-Account Oracle Cloud Infrastructure       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Account 1      Account 2      Account 3     Account 4  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚instance-20250216-2117 â”‚   â”‚instance-20250209-1502 â”‚   â”‚instance-20250209-1504 â”‚   â”‚instance-20250306-1735 â”‚â”‚
â”‚  â”‚Control  â”‚   â”‚Database â”‚   â”‚  App    â”‚   â”‚Storage  â”‚â”‚
â”‚  â”‚ Plane   â”‚   â”‚  Node   â”‚   â”‚  Node   â”‚   â”‚  Node   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    Tailscale VPN (í•„ìˆ˜)                 â”‚
â”‚                    100.64.0.0/10 Network                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ ëª©ì°¨
1. [ì‚¬ì „ ì¤€ë¹„](#1-ì‚¬ì „-ì¤€ë¹„)
2. [í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”](#2-í´ëŸ¬ìŠ¤í„°-ì´ˆê¸°í™”)
3. [Master ë…¸ë“œ ì„¤ì •](#3-master-ë…¸ë“œ-ì„¤ì •)
4. [Worker ë…¸ë“œ ì„¤ì •](#4-worker-ë…¸ë“œ-ì„¤ì •)
5. [ë„¤íŠ¸ì›Œí¬ ë° ìŠ¤í† ë¦¬ì§€](#5-ë„¤íŠ¸ì›Œí¬-ë°-ìŠ¤í† ë¦¬ì§€)
6. [ì¸ì¦ì„œ ë° ì¸ê·¸ë ˆìŠ¤](#6-ì¸ì¦ì„œ-ë°-ì¸ê·¸ë ˆìŠ¤)
7. [ê³ ê°€ìš©ì„± Data Plane ë°°í¬](#7-ê³ ê°€ìš©ì„±-data-plane-ë°°í¬)
8. [ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜](#8-ëª¨ë‹ˆí„°ë§-ë°-ìš´ì˜)

---

## 1. ì‚¬ì „ ì¤€ë¹„

### 1.1 Oracle Cloud ARM64 4-Node ê³ ê°€ìš©ì„± êµ¬ì„±

| ë…¸ë“œ | í˜¸ìŠ¤íŠ¸ëª… | Tailscale IP | ì—­í•  | ì»´í¬ë„ŒíŠ¸ ë°°ì¹˜ |
|------|---------|-------------|------|-------------|
| Node 1 | instance-20250216-2117 | 100.64.0.1 | Control Plane | â€¢ Kubernetes API Server<br>â€¢ etcd<br>â€¢ Controller Manager<br>â€¢ Scheduler<br>â€¢ Higress Gateway & Console |
| Node 2 | instance-20250209-1502 | 100.64.0.2 | Worker (DB) | â€¢ MongoDB Primary<br>â€¢ Data Plane Server (Primary) |
| Node 3 | instance-20250209-1504 | 100.64.0.3 | Worker (App) | â€¢ MongoDB Secondary<br>â€¢ Data Plane Server (Secondary)<br>â€¢ Data Plane Web |
| Node 4 | instance-20250306-1735 | 100.64.0.6 | Worker (Storage) | â€¢ MinIO<br>â€¢ Prometheus<br>â€¢ Grafana<br>â€¢ Backup Services |

#### ë¦¬ì†ŒìŠ¤ ì‚¬ì–‘ (ê° ë…¸ë“œ)
- **CPU**: 4 OCPU (ARM Ampere A1)
- **RAM**: 24GB
- **Disk**: 200GB Block Volume
- **Network**: 1Gbps

### 1.2 í•„ìš” ë„êµ¬ ë° ë²„ì „
- **OS**: Oracle Linux 8.x (ARM64)
- **Kubernetes**: v1.28.15
- **Containerd**: v1.7.0+
- **Cilium**: v1.17.5 (CNI)
- **Helm**: v3.15.0
- **Higress**: v2.1.6 (Ingress Controller)

### 1.3 ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- **Tailscale**: P2P VPN (**í•„ìˆ˜** - ì—¬ëŸ¬ Oracle ê³„ì • ê°„ í†µì‹ )
- **Pod Network**: 10.244.0.0/16
- **Service Network**: 10.96.0.0/12
- **NodePort Range**: 30000-32767

### 1.4 ë„ë©”ì¸ ë° ì¸ì¦ì„œ
- ë„ë©”ì¸: `prod.scraping.run`
- Cloudflare DNS (DNS-01 Challenge)
- Let's Encrypt ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ

---

## 2. í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”

### 2.1 ê¸°ì¡´ í´ëŸ¬ìŠ¤í„° ë¦¬ì…‹ (í•„ìš”í•œ ê²½ìš°)

ëª¨ë“  ë…¸ë“œì—ì„œ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸°ì¡´ Kubernetes ì„¤ì •ì„ ì™„ì „íˆ ì œê±°í•©ë‹ˆë‹¤:

```bash
#!/bin/bash
# reset-k8s.sh ì‹¤í–‰
chmod +x reset-k8s.sh
./reset-k8s.sh
```

### 2.2 ë…¸ë“œë³„ í˜¸ìŠ¤íŠ¸ë„¤ì„ ì„¤ì •

```bash
# Node 1 (Master)
sudo hostnamectl set-hostname instance-20250216-2117

# Node 2 (Worker)
sudo hostnamectl set-hostname instance-20250209-1502

# Node 3 (Worker)
sudo hostnamectl set-hostname instance-20250209-1504

# Node 4 (Worker)
sudo hostnamectl set-hostname instance-20250306-1735
```

### 2.3 Tailscale ì„¤ì • í™•ì¸ (í•„ìˆ˜)

```bash
# Tailscale ìƒíƒœ í™•ì¸
tailscale status

# Tailscale IP í™•ì¸
tailscale ip -4

# ë‹¤ë¥¸ ë…¸ë“œì™€ ì—°ê²° í…ŒìŠ¤íŠ¸
tailscale ping instance-20250216-2117  # ê° ë…¸ë“œì—ì„œ ì‹¤í–‰
tailscale ping instance-20250209-1502
tailscale ping instance-20250209-1504
tailscale ping instance-20250306-1735
```

### 2.4 /etc/hosts íŒŒì¼ ì„¤ì • (ëª¨ë“  ë…¸ë“œ)

```bash
# Tailscale IPë¡œ ì„¤ì • (í•„ìˆ˜)
# ê° ë…¸ë“œì˜ ì‹¤ì œ Tailscale IPë¡œ ë³€ê²½í•˜ì„¸ìš”
cat <<EOF | sudo tee -a /etc/hosts
100.64.0.1 instance-20250216-2117
100.64.0.2 instance-20250209-1502
100.64.0.3 instance-20250209-1504
100.64.0.4 instance-20250306-1735
EOF

# ê° ë…¸ë“œì˜ ì‹¤ì œ Tailscale IP í™•ì¸ ë°©ë²•:
# tailscale statusë¡œ ê° ë…¸ë“œì˜ IP í™•ì¸ í›„ ìœ„ ê°’ì„ ìˆ˜ì •
```

---

## 3. Master ë…¸ë“œ ì„¤ì •

### 3.1 Oracle Cloud ë°©í™”ë²½ ì„¤ì • (í•„ìˆ˜)

Oracle CloudëŠ” ê¸°ë³¸ì ìœ¼ë¡œ OCI Security Listë¥¼ ì‚¬ìš©í•˜ë©°, ë¡œì»¬ ë°©í™”ë²½(iptables/firewalld)ì€ ë¹„í™œì„±í™”í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.

```bash
# ë¡œì»¬ ë°©í™”ë²½ ë¹„í™œì„±í™” (Oracle Cloud ê¶Œì¥)
# firewalld ì¤‘ì§€ ë° ë¹„í™œì„±í™”
sudo systemctl stop firewalld 2>/dev/null || true
sudo systemctl disable firewalld 2>/dev/null || true

# iptables ì„œë¹„ìŠ¤ ì¤‘ì§€ (Oracle Linux)
sudo systemctl stop iptables 2>/dev/null || true
sudo systemctl disable iptables 2>/dev/null || true

# ë°©í™”ë²½ ìƒíƒœ í™•ì¸
echo "Firewall status:"
sudo systemctl status firewalld --no-pager 2>/dev/null || echo "firewalld not installed"
sudo systemctl status iptables --no-pager 2>/dev/null || echo "iptables service not installed"

# âš ï¸ ì¤‘ìš”: Oracle CloudëŠ” OCI Consoleì˜ Security Listë¡œ ë°©í™”ë²½ì„ ê´€ë¦¬í•©ë‹ˆë‹¤!
```

### OCI Consoleì—ì„œ Security List ì„¤ì • (í•„ìˆ˜!)

Oracle Cloud Consoleì— ë¡œê·¸ì¸í•˜ì—¬ ë‹¤ìŒ ì„¤ì •ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. **Networking â†’ Virtual Cloud Networks â†’ í•´ë‹¹ VCN ì„ íƒ**
2. **Security Lists â†’ Default Security List ë˜ëŠ” Custom Security List ì„ íƒ**
3. **Ingress Rules ì¶”ê°€:**

| Source | Protocol | Port Range | Description |
|--------|----------|------------|-------------|
| 0.0.0.0/0 | TCP | 22 | SSH |
| 0.0.0.0/0 | TCP | 80 | HTTP |
| 0.0.0.0/0 | TCP | 443 | HTTPS |
| 0.0.0.0/0 | TCP | 6443 | Kubernetes API |
| 0.0.0.0/0 | TCP | 30000-32767 | NodePort Services |
| VCN CIDR | TCP | 2379-2380 | etcd |
| VCN CIDR | TCP | 10250-10252 | kubelet |
| VCN CIDR | TCP | 4240 | Cilium health |
| VCN CIDR | TCP | 4244 | Cilium Hubble |
| VCN CIDR | UDP | 8472 | Cilium VXLAN |
| VCN CIDR | UDP | 51871 | WireGuard (Cilium) |
| 0.0.0.0/0 | UDP | 51820 | Tailscale |

```bash
# Security List ì„¤ì • í™•ì¸ ëª…ë ¹
echo "âš ï¸  OCI Consoleì—ì„œ ìœ„ì˜ í¬íŠ¸ë“¤ì´ Security Listì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!"
echo "ì„¤ì • ê²½ë¡œ: OCI Console â†’ Networking â†’ VCN â†’ Security Lists â†’ Ingress Rules"
```

### 3.2 ì‹œìŠ¤í…œ ì¤€ë¹„ì‚¬í•­ (ëª¨ë“  ë…¸ë“œì—ì„œ ì‹¤í–‰)

```bash
# 0. ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€
# Oracle Linux 8 / RHEL 8 / CentOS 8
sudo dnf update -y
sudo dnf install -y curl wget git vim net-tools telnet bind-utils

# Ubuntu/Debian (ì°¸ê³ ìš©)
# sudo apt-get update
# sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common

# 1. Swap ë¹„í™œì„±í™”
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# 2. ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 3. ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì„¤ì •
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# 4. Containerd ì„¤ì¹˜
# Oracle Linux 8 / RHEL 8
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io

# Ubuntu/Debian (ì°¸ê³ ìš©)
# sudo apt-get update
# sudo apt-get install -y containerd

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl enable --now containerd
sudo systemctl restart containerd

# 5. Kubernetes íŒ¨í‚¤ì§€ ì„¤ì¹˜
# Oracle Linux 8 / RHEL 8
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.28/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

sudo dnf install -y kubelet-1.28.15 kubeadm-1.28.15 kubectl-1.28.15 --disableexcludes=kubernetes
sudo systemctl enable --now kubelet

# Ubuntu/Debian (ì°¸ê³ ìš©)
# sudo mkdir -p /etc/apt/keyrings
# curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
# echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
# sudo apt-get update
# sudo apt-get install -y kubelet=1.28.15-1.1 kubeadm=1.28.15-1.1 kubectl=1.28.15-1.1
# sudo apt-mark hold kubelet kubeadm kubectl

# 6. crictl ì„¤ì¹˜ (ì»¨í…Œì´ë„ˆ ë””ë²„ê¹…ìš©)
VERSION="v1.28.0"
# ARM64 ì•„í‚¤í…ì²˜ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
if [ "$(uname -m)" = "aarch64" ]; then
    ARCH="arm64"
else
    ARCH="amd64"
fi
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-${ARCH}.tar.gz
sudo tar zxvf crictl-$VERSION-linux-${ARCH}.tar.gz -C /usr/local/bin
rm -f crictl-$VERSION-linux-${ARCH}.tar.gz
```

### 3.3 Master ë…¸ë“œ ì´ˆê¸°í™” (instance-20250216-2117ì—ì„œë§Œ ì‹¤í–‰)

```bash
# Tailscale IP ê°€ì ¸ì˜¤ê¸° (í•„ìˆ˜)
MASTER_IP=$(tailscale ip -4 | head -n 1)
if [ -z "$MASTER_IP" ]; then
    echo "ERROR: Tailscale IPë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Tailscaleì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    echo "tailscale status ëª…ë ¹ìœ¼ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
    exit 1
fi
echo "Master Tailscale IP: $MASTER_IP"

# Public IPëŠ” ì„ íƒì‚¬í•­ (í•„ìš”ì‹œ ì‚¬ìš©)
PUBLIC_IP=$(curl -s ifconfig.me)
echo "Public IP (ì„ íƒì‚¬í•­): $PUBLIC_IP"

# kubeadm ì„¤ì • íŒŒì¼ ìƒì„±
cat <<EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: $MASTER_IP
  bindPort: 6443
skipPhases:
  - addon/kube-proxy
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.15
networking:
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
apiServer:
  certSANs:
    - 127.0.0.1
    - $MASTER_IP
    - $PUBLIC_IP
    - localhost
    - kubernetes
    - kubernetes.default
    - kubernetes.default.svc
    - kubernetes.default.svc.cluster.local
  extraArgs:
    enable-admission-plugins: NodeRestriction,ResourceQuota
controllerManager:
  extraArgs:
    bind-address: 0.0.0.0
scheduler:
  extraArgs:
    bind-address: 0.0.0.0
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true
cgroupDriver: systemd
EOF

# kubeadm ì´ˆê¸°í™” ì‹¤í–‰
sudo kubeadm init --config=kubeadm-config.yaml

# kubectl ì„¤ì •
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# í´ëŸ¬ìŠ¤í„° êµ¬ì„± ì„ íƒ
echo "==========================================="
echo "í´ëŸ¬ìŠ¤í„° êµ¬ì„±ì„ ì„ íƒí•˜ì„¸ìš”:"
echo "1) ë‹¨ì¼ ë…¸ë“œ (ë§ˆìŠ¤í„°ê°€ ì›Œì»¤ ì—­í• ë„ ìˆ˜í–‰)"
echo "2) ë‹¤ì¤‘ ë…¸ë“œ (ë§ˆìŠ¤í„° ì „ìš©, ì›Œì»¤ ë…¸ë“œ ë³„ë„)"
echo "==========================================="

# Option 1: ë‹¨ì¼ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (ë§ˆìŠ¤í„°ì— ì›Œí¬ë¡œë“œ ë°°í¬ í—ˆìš©)
# ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì ì´ê±°ë‚˜ ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ì í•©
echo "ë‹¨ì¼ ë…¸ë“œë¡œ êµ¬ì„±í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:"
echo "kubectl taint nodes --all node-role.kubernetes.io/control-plane-"
echo "kubectl label nodes --all node-role.kubernetes.io/worker=true"

# Option 2: ë‹¤ì¤‘ ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (í”„ë¡œë•ì…˜ ê¶Œì¥)
# ë§ˆìŠ¤í„°ëŠ” ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ë§Œ, ì›Œì»¤ëŠ” ì›Œí¬ë¡œë“œ ì‹¤í–‰
echo ""
echo "ë‹¤ì¤‘ ë…¸ë“œë¡œ êµ¬ì„±í•˜ë ¤ë©´:"
echo "1. ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ "
echo "2. ì›Œì»¤ ë…¸ë“œì—ì„œ ì•„ë˜ join ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”"

# Join í† í° ìƒì„± (24ì‹œê°„ ìœ íš¨)
echo ""
echo "ì›Œì»¤ ë…¸ë“œ ì¶”ê°€ë¥¼ ìœ„í•œ join ëª…ë ¹:"
kubeadm token create --print-join-command

# í† í° ëª©ë¡ í™•ì¸
kubeadm token list
```

### 3.3 kubelet ì¸ì¦ì„œ ìë™ ìŠ¹ì¸ ì„¤ì • (Masterì—ì„œ ì‹¤í–‰)

```bash
# kubelet-serving ì¸ì¦ì„œ ìë™ ìŠ¹ì¸ ì„¤ì •
# ì´ ì„¤ì •ì´ ì—†ìœ¼ë©´ Worker ë…¸ë“œ Join í›„ TLS ì—ëŸ¬ ë°œìƒ
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubelet-serving-cert-approver
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
EOF

echo "âœ… kubelet ì¸ì¦ì„œ ìë™ ìŠ¹ì¸ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

### 3.4 ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
# í˜„ì¬ ë…¸ë“œ ìƒíƒœ (Masterë§Œ ìˆëŠ” ìƒíƒœ)
kubectl get nodes

# ë…¸ë“œê°€ NotReady ìƒíƒœë©´ CNI ì„¤ì¹˜ í•„ìš” (ì„¹ì…˜ 5.1 ì°¸ì¡°)
```

## 4. Worker ë…¸ë“œ ì„¤ì •

ê° Worker ë…¸ë“œ(instance-20250209-1502, instance-20250209-1504, instance-20250306-1735)ì—ì„œ ì•„ë˜ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

### 4.1 Worker ë…¸ë“œ Join (instance-20250209-1502, instance-20250209-1504, instance-20250306-1735ì—ì„œ ê°ê° ì‹¤í–‰)

```bash
# 1. Tailscale IP í™•ì¸ (í•„ìˆ˜)
TAIL_IP=$(tailscale ip -4 | head -n 1)
if [ -z "$TAIL_IP" ]; then
    echo "ERROR: Tailscale IPë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Tailscaleì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
    exit 1
fi
echo "Worker Tailscale IP: $TAIL_IP"

# 2. kubeletì— Tailscale IP ì„¤ì •
# ì¤‘ìš”: kubeadm joinì€ --node-ip í”Œë˜ê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
# ëŒ€ì‹  /etc/default/kubeletì— ì„¤ì •í•´ì•¼ í•¨
cat <<EOF | sudo tee /etc/default/kubelet
KUBELET_EXTRA_ARGS="--node-ip=${TAIL_IP}"
EOF

# systemd ì¬ë¡œë“œ
sudo systemctl daemon-reload

# 3. Masterì—ì„œ ë°›ì€ join ëª…ë ¹ ì‹¤í–‰
# Master ë…¸ë“œì—ì„œ 'kubeadm token create --print-join-command' ì‹¤í–‰í•˜ì—¬ ì–»ì€ ëª…ë ¹ ì‚¬ìš©
# ì˜ˆì‹œ (ì‹¤ì œ í† í°ê³¼ í•´ì‹œë¡œ êµì²´):
sudo kubeadm join 100.64.0.1:6443 \
  --token abcdef.1234567890abcdef \
  --discovery-token-ca-cert-hash sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

# 4. Join í›„ kubelet ìƒíƒœ í™•ì¸
sudo systemctl status kubelet --no-pager
```

### 4.2 Join ë¬¸ì œ í•´ê²°

#### Worker ë…¸ë“œê°€ Joinë˜ì§€ ì•ŠëŠ” ê²½ìš°:

```bash
# 1. Masterì—ì„œ í† í° í™•ì¸
kubeadm token list

# í† í°ì´ ë§Œë£Œëœ ê²½ìš° ìƒˆë¡œ ìƒì„±
kubeadm token create --print-join-command

# 2. Worker ë…¸ë“œì—ì„œ kubelet ìƒíƒœ í™•ì¸
sudo systemctl status kubelet
sudo journalctl -xeu kubelet | tail -50

# 3. Worker ë…¸ë“œì—ì„œ Tailscale ì—°ê²° í™•ì¸
tailscale ping instance-20250216-2117
# ë˜ëŠ” Masterì˜ Tailscale IPë¡œ ì§ì ‘ ping
ping 100.64.0.1

# 4. Worker ë…¸ë“œ ì™„ì „ ì´ˆê¸°í™” (leftover ì„¤ì • ì œê±°)
# ê¸°ì¡´ ì„¤ì •ì´ ë‚¨ì•„ìˆì–´ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš° ì‚¬ìš©
sudo kubeadm reset -f
sudo systemctl stop kubelet
sudo rm -rf /etc/kubernetes /var/lib/kubelet /var/lib/etcd
sudo rm -rf /etc/cni/net.d /var/lib/cni/
sudo rm -f /etc/default/kubelet

# iptables ì •ë¦¬
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X

# kubelet ì¬ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 5. Tailscale IPë¡œ ë‹¤ì‹œ Join
TAIL_IP=$(tailscale ip -4)
echo "KUBELET_EXTRA_ARGS=\"--node-ip=${TAIL_IP}\"" | sudo tee /etc/default/kubelet
sudo systemctl daemon-reload

# Masterì—ì„œ ë°›ì€ join ëª…ë ¹ ì¬ì‹¤í–‰
```

### 4.3 Join ìƒíƒœ í™•ì¸ (Masterì—ì„œ ì‹¤í–‰)

```bash
# ë…¸ë“œ Join ìƒíƒœ ëª¨ë‹ˆí„°ë§
kubectl get nodes -w

# ëª¨ë“  ë…¸ë“œê°€ í‘œì‹œë  ë•Œê¹Œì§€ ëŒ€ê¸°
# ì˜ˆìƒ ê²°ê³¼:
# NAME       STATUS     ROLES           AGE   VERSION
# instance-20250216-2117   NotReady   control-plane   10m   v1.28.15
# instance-20250209-1502   NotReady   <none>          1m    v1.28.15
# instance-20250209-1504   NotReady   <none>          1m    v1.28.15
# instance-20250306-1735   NotReady   <none>          1m    v1.28.15

# NotReady ìƒíƒœëŠ” CNIê°€ ì•„ì§ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ì„œì„ (ì •ìƒ)
# ì„¹ì…˜ 5.1ì—ì„œ Cilium ì„¤ì¹˜ í›„ Readyë¡œ ë³€ê²½ë¨

# CSR (Certificate Signing Request) í™•ì¸
kubectl get csr

# Pending ìƒíƒœì˜ CSRì´ ìˆë‹¤ë©´ ìŠ¹ì¸ (ìë™ ìŠ¹ì¸ ì„¤ì •ì´ ì•ˆ ëœ ê²½ìš°)
kubectl get csr -o name | xargs kubectl certificate approve
```

### 4.4 ë…¸ë“œ ë ˆì´ë¸” ì„¤ì • (Masterì—ì„œ ì‹¤í–‰)

**ì¤‘ìš”**: ëª¨ë“  Worker ë…¸ë“œê°€ í´ëŸ¬ìŠ¤í„°ì— Joinëœ í›„ì— ì‹¤í–‰í•˜ì„¸ìš”.

```bash
# ë…¸ë“œ í™•ì¸ (4ê°œ ë…¸ë“œê°€ ëª¨ë‘ ë³´ì—¬ì•¼ í•¨)
kubectl get nodes

# ë…¸ë“œê°€ 4ê°œ ë¯¸ë§Œì¸ ê²½ìš°:
# 1. Worker ë…¸ë“œì—ì„œ join ëª…ë ¹ì„ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸
# 2. ê° Worker ë…¸ë“œì—ì„œ 'sudo systemctl status kubelet' í™•ì¸
# 3. Masterì—ì„œ ìƒˆ í† í° ìƒì„±: kubeadm token create --print-join-command

# Worker ë…¸ë“œ ë ˆì´ë¸” ì„¤ì • (ë…¸ë“œê°€ ì¡´ì¬í•  ë•Œë§Œ ì‹¤í–‰)
kubectl label nodes instance-20250209-1502 node-role.kubernetes.io/worker=true --overwrite
kubectl label nodes instance-20250209-1504 node-role.kubernetes.io/worker=true --overwrite
kubectl label nodes instance-20250306-1735 node-role.kubernetes.io/worker=true --overwrite

# ìš©ë„ë³„ ë ˆì´ë¸” ì„¤ì •
kubectl label nodes instance-20250209-1502 node-role=database --overwrite
kubectl label nodes instance-20250209-1504 node-role=application --overwrite
kubectl label nodes instance-20250306-1735 node-role=storage --overwrite

# ë ˆì´ë¸” í™•ì¸
kubectl get nodes --show-labels
```

---

## 5. ë„¤íŠ¸ì›Œí¬ ë° ìŠ¤í† ë¦¬ì§€

### 5.1 Cilium CNI ì„¤ì¹˜

#### Helm ì„¤ì¹˜
```bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
helm repo add cilium https://helm.cilium.io/
helm repo update
```

#### Cilium ì„¤ì¹˜ (4ë…¸ë“œ ìµœì í™”)

```bash
# Cilium values íŒŒì¼ ìƒì„±
cat <<EOF > cilium-values.yaml
routingMode: tunnel
tunnelProtocol: vxlan
kubeProxyReplacement: true
bpf:
  masquerade: true
  tproxy: true
ipam:
  mode: kubernetes
mtu: 1280
k8sServiceHost: $MASTER_IP
k8sServicePort: 6443
encryption:
  enabled: false
hubble:
  enabled: true
  tls:
    enabled: false
  relay:
    enabled: true
    tls:
      server:
        enabled: false
      client:
        enabled: false
  ui:
    enabled: false
operator:
  replicas: 1
ipv4:
  enabled: true
ipv6:
  enabled: false
EOF

# Helmìœ¼ë¡œ ì„¤ì¹˜
helm upgrade --install cilium cilium/cilium \
  --namespace kube-system \
  --version 1.17.5 \
  --values cilium-values.yaml
```

#### Cilium ìƒíƒœ í™•ì¸

```bash
# Cilium CLI ì„¤ì¹˜
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}

# ìƒíƒœ í™•ì¸
cilium status --wait
```

---

### 5.2 ìŠ¤í† ë¦¬ì§€ ì„¤ì •

#### Helm Repository ì¶”ê°€

```bash
# í•„ìˆ˜ Helm repositories ì¶”ê°€
helm repo add openebs https://openebs.github.io/charts
helm repo add jetstack https://charts.jetstack.io
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm repo add higress https://higress.io/helm-charts
helm repo add apecloud https://apecloud.github.io/helm-charts
helm repo update
```

#### OpenEBS ì„¤ì¹˜ (Local PV)

```bash
# OpenEBS values íŒŒì¼ ìƒì„±
cat <<EOF > openebs-values.yaml
engines:
  replicated:
    mayastor:
      enabled: false
  local:
    hostpath:
      enabled: true
      basePath: /var/openebs/local
helper:
  imagePrefix: "openebs/"
analytics:
  enabled: false
defaultStorageConfig:
  enabled: true
EOF

# OpenEBS ì„¤ì¹˜
helm upgrade --install openebs openebs/openebs \
  --namespace openebs \
  --create-namespace \
  --version 3.10.0 \
  --values openebs-values.yaml

# ê¸°ë³¸ StorageClass ì„¤ì •
kubectl patch storageclass openebs-hostpath \
  -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# StorageClass í™•ì¸
kubectl get storageclass
```

### 5.3 cert-manager ì„¤ì¹˜ (DNS ì±Œë¦°ì§€ ì¤€ë¹„)

```bash
# cert-manager CRDs ë¨¼ì € ì„¤ì¹˜
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.2/cert-manager.crds.yaml

# cert-manager values íŒŒì¼
cat <<EOF > cert-manager-values.yaml
crds:
  enabled: false  # ì´ë¯¸ ì„¤ì¹˜í•¨
  keep: true
global:
  leaderElection:
    namespace: cert-manager
prometheus:
  enabled: false
webhook:
  timeoutSeconds: 30
EOF

# cert-manager ì„¤ì¹˜
helm upgrade --install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.16.2 \
  --values cert-manager-values.yaml

# ì„¤ì¹˜ í™•ì¸ (ëª¨ë“  Podê°€ Running ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
kubectl -n cert-manager wait --for=condition=ready pod --all --timeout=300s
```

### 5.4 metrics-server ì„¤ì¹˜

```bash
# metrics-server ì„¤ì¹˜ (Tailscale í™˜ê²½ìš© ì„¤ì •)
helm upgrade --install metrics-server metrics-server/metrics-server \
  --namespace kube-system \
  --set 'args={--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}'
```

### 5.5 Higress ì„¤ì¹˜ (ìµœì‹  v2.1.6) - Console í¬í•¨ (Master ë…¸ë“œì—ë§Œ ë°°í¬)

```bash
# Higress values íŒŒì¼ ìƒì„± (Console í¬í•¨, Master ë…¸ë“œ ì „ìš©)
cat <<EOF > higress-values.yaml
higress-core:
  gateway:
    replicas: 1
    service:
      type: NodePort
      nodePorts:
        http: 30080
        https: 30443
    httpsPort: 443  # HTTPS í¬íŠ¸ í™œì„±í™”
    resources:
      limits:
        cpu: 1000m
        memory: 1024Mi
      requests:
        cpu: 100m
        memory: 128Mi
    hostNetwork: true
    # Master ë…¸ë“œì—ë§Œ ë°°í¬
    nodeSelector:
      node-role.kubernetes.io/control-plane: ""
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  controller:
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    # Master ë…¸ë“œì—ë§Œ ë°°í¬
    nodeSelector:
      node-role.kubernetes.io/control-plane: ""
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

# Higress Console ì„¤ì •
higress-console:
  enabled: true
  domain: higress.prod.scraping.run
  service:
    type: ClusterIP
    port: 8080
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  # ê´€ë¦¬ì ê³„ì • ì„¤ì •
  adminUsername: admin
  adminPassword: aldhr1011

global:
  ingressClass: nginx  # nginx í˜¸í™˜ ëª¨ë“œ
  enableStatus: false
  enableGatewayAPI: false
  disableAlpnH2: false
  enableIstioAPI: true
  enableSRDS: true
  # ARM64 í˜¸í™˜ì„±
  arch: arm64
EOF

# Higress ì„¤ì¹˜ (v2.1.6) with Console
helm upgrade --install higress higress/higress \
  --namespace higress-system \
  --create-namespace \
  --version 2.1.6 \
  --values higress-values.yaml \
  --set higress-console.domain=higress.prod.scraping.run \
  --render-subchart-notes \
  --wait

# ì„¤ì¹˜ í™•ì¸ (ëª¨ë“  Podê°€ Running ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
kubectl wait --for=condition=ready pod --all -n higress-system --timeout=300s

# ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œë¥¼ higress-system ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ë³µì‚¬
echo "ì¸ì¦ì„œ ë³µì‚¬ ì¤‘..."
kubectl get secret prod-scraping-run-wildcards-tls -n data-plane-system -o yaml | \
  sed 's/namespace: data-plane-system/namespace: higress-system/' | \
  kubectl apply -f -

# Higress Console Ingress ìƒì„±
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: higress-console
  namespace: higress-system
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-cloudflare"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - higress.prod.scraping.run
    secretName: prod-scraping-run-wildcards-tls
  rules:
  - host: higress.prod.scraping.run
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: higress-console
            port:
              number: 8080
EOF

# Console ì ‘ì† ì •ë³´ ì¶œë ¥
echo ""
echo "========================================="
echo "Higress Console ì„¤ì¹˜ ì™„ë£Œ!"
echo "========================================="
echo "Console URL: https://gw.prod.scraping.run"
echo "Username: admin"
echo "Password: aldhr1011"
echo ""
echo "Grafana (ë‚´ì¥ ëª¨ë‹ˆí„°ë§): https://gw.prod.scraping.run/grafana"
echo "========================================="

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get pods -n higress-system
kubectl get svc -n higress-system
kubectl get ingress -n higress-system
```

> ğŸ’¡ **Higress Console ê¸°ëŠ¥**:
> - **Gateway ê´€ë¦¬**: Ingress ê·œì¹™ ì„¤ì • ë° ë¼ìš°íŒ… ì •ì±… ê´€ë¦¬
> - **í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬**: WAF, Rate Limiting, ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì„¤ì •
> - **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**: ìš”ì²­ë¥ , ì—ëŸ¬ìœ¨, P95/P99 ë ˆì´í„´ì‹œ í™•ì¸
> - **ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬**: Kubernetes ì„œë¹„ìŠ¤ ìë™ ê°ì§€ ë° ì„¤ì •

> âš ï¸ **ì¤‘ìš”**: DNSì— `gw.prod.scraping.run` A ë ˆì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ Console ì ‘ì†ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### 5.6 KubeBlocks ì„¤ì¹˜ (ARM64 ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬)

```bash
# KubeBlocks CRDs ë¨¼ì € ì„¤ì¹˜
kubectl create -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.0/kubeblocks_crds.yaml

# CRD ì„¤ì¹˜ í™•ì¸ (í•„ìˆ˜)
kubectl wait --for condition=established --timeout=120s crd/clusters.apps.kubeblocks.io

# KubeBlocks values íŒŒì¼ ìƒì„±
cat <<EOF > kubeblocks-values.yaml
image:
  registry: docker.io
  pullPolicy: IfNotPresent
monitoring:
  enabled: false  # ë¦¬ì†ŒìŠ¤ ì ˆì•½
multiCluster:
  enabled: false  # ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°
addonController:
  enabled: true
dataProtection:
  enabled: false  # ë¦¬ì†ŒìŠ¤ ì ˆì•½
# ARM64 ìµœì í™” ë¦¬ì†ŒìŠ¤
resources:
  limits:
    cpu: 1000m
    memory: 1024Mi
  requests:
    cpu: 100m
    memory: 128Mi
# ë‹¨ì¼ ë…¸ë“œ í†¨ëŸ¬ë ˆì´ì…˜
tolerations:
  - operator: Exists
EOF

# KubeBlocks ì„¤ì¹˜
helm upgrade --install kubeblocks apecloud/kubeblocks \
  --namespace kb-system \
  --create-namespace \
  --version 0.9.0 \
  --values kubeblocks-values.yaml \
  --wait

# ì„¤ì¹˜ í™•ì¸
kubectl -n kb-system get pods
kubectl get crd | grep kubeblocks

# MongoDB Addon ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
# KubeBlocksê°€ ì™„ì „íˆ ì¤€ë¹„ëœ í›„ ì‹¤í–‰
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=kubeblocks -n kb-system --timeout=300s

# MongoDB ì§€ì› í™•ì¸
kubectl get clusterdefinition mongodb
kubectl get clusterversion mongodb-5.0
```

---

## 6. ì¸ì¦ì„œ ë° ì¸ê·¸ë ˆìŠ¤

### 6.1 TLS ì¸ì¦ì„œ ì„¤ì •

#### Cloudflare API Token Secret ìƒì„±

```bash
# Cloudflare API í† í° ì‹œí¬ë¦¿ ìƒì„±
# ì£¼ì˜: API í† í°ì€ ì‹¤ì œ í† í°ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”
kubectl -n cert-manager create secret generic cloudflare-api-token-secret \
  --from-literal=api-token="82LiIqSa8zZUkFNyNKgd6Xd5VIUbL9j7T5Vc4TN1"

# Cloudflare API í† í° ìš”êµ¬ì‚¬í•­:
# - Zone:Zone:Read
# - Zone:DNS:Edit
# - í•´ë‹¹ ë„ë©”ì¸ì˜ Zoneì— ëŒ€í•œ ê¶Œí•œ
```

#### ClusterIssuer ìƒì„±

```bash
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-cloudflare
spec:
  acme:
    email: junsik.park@gmail.com
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-cloudflare-account-key
    solvers:
    - selector:
        dnsZones:
        - "prod.scraping.run"
      dns01:
        cloudflare:
          apiTokenSecretRef:
            name: cloudflare-api-token-secret
            key: api-token
EOF

# Staging í™˜ê²½ í…ŒìŠ¤íŠ¸ìš© (Rate Limit íšŒí”¼)
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    email: junsik.park@gmail.com
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-staging-account-key
    solvers:
    - selector:
        dnsZones:
        - "prod.scraping.run"
      dns01:
        cloudflare:
          apiTokenSecretRef:
            name: cloudflare-api-token-secret
            key: api-token
EOF
```

#### ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ ìƒì„±

```bash
# data-plane-system ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace data-plane-system

# ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ ìƒì„± (prod.scraping.run)
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: prod-scraping-run-wildcards
  namespace: data-plane-system
spec:
  secretName: prod-scraping-run-wildcards-tls
  issuerRef:
    name: letsencrypt-cloudflare
    kind: ClusterIssuer
  dnsNames:
  - "prod.scraping.run"
  - "*.prod.scraping.run"
EOF

# ì¸ì¦ì„œ ë°œê¸‰ ìƒíƒœ í™•ì¸
kubectl -n data-plane-system get certificate --watch

# ë³„ë„ í„°ë¯¸ë„ì—ì„œ ìƒì„¸ ìƒíƒœ í™•ì¸
kubectl -n data-plane-system get certificaterequest
kubectl -n data-plane-system get order
kubectl -n data-plane-system get challenge

# ì¸ì¦ì„œ ìƒíƒœ ìƒì„¸ í™•ì¸
kubectl -n data-plane-system describe certificate prod-scraping-run-wildcards

# DNS ë ˆì½”ë“œ í™•ì¸ (DNS-01 ì±Œë¦°ì§€)
dig _acme-challenge.prod.scraping.run TXT +short

# ì¸ì¦ì„œ ë°œê¸‰ ì™„ë£Œ í™•ì¸ (Ready = True)
kubectl -n data-plane-system get certificate
# NAME                          READY   SECRET                             AGE
# prod-scraping-run-wildcards   True    prod-scraping-run-wildcards-tls   5m
```

#### ë¬¸ì œ í•´ê²°

##### ì™€ì¼ë“œì¹´ë“œ ë„ë©”ì¸ ì¤‘ë³µ ì—ëŸ¬
```bash
# ì—ëŸ¬: "Domain name is redundant with a wildcard domain"
# í•´ê²°: ì™€ì¼ë“œì¹´ë“œ(*.domain.com)ì™€ ê°œë³„ ì„œë¸Œë„ë©”ì¸ì„ í•¨ê»˜ ìš”ì²­í•˜ì§€ ì•ŠìŒ
# ì˜¬ë°”ë¥¸ ì„¤ì •: domain.com + *.domain.comë§Œ ì‚¬ìš©
```

##### Rate Limit ì—ëŸ¬
```bash
# Let's Encrypt Rate Limitì— ê±¸ë¦° ê²½ìš°
# 1. Staging í™˜ê²½ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í…ŒìŠ¤íŠ¸
kubectl -n data-plane-system patch certificate prod-scraping-run-wildcards \
  --type='json' -p='[{"op": "replace", "path": "/spec/issuerRef/name", "value":"letsencrypt-staging"}]'

# 2. í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ Productionìœ¼ë¡œ ë³€ê²½
kubectl -n data-plane-system patch certificate prod-scraping-run-wildcards \
  --type='json' -p='[{"op": "replace", "path": "/spec/issuerRef/name", "value":"letsencrypt-cloudflare"}]'
```

##### CSR ìŠ¹ì¸ ëŒ€ê¸°
```bash
# Challengeê°€ Pending ìƒíƒœë¡œ ë©ˆì¶˜ ê²½ìš°
# 1. DNS ì „íŒŒ í™•ì¸
dig _acme-challenge.prod.scraping.run TXT +short

# 2. cert-manager ë¡œê·¸ í™•ì¸
kubectl logs -n cert-manager deploy/cert-manager --tail=50

# 3. Cloudflare API í† í° ê¶Œí•œ í™•ì¸
# - Zone:Zone:Read
# - Zone:DNS:Edit ê¶Œí•œ í•„ìš”
```

##### ì¸ì¦ì„œ ì¬ë°œê¸‰
```bash
# ì¸ì¦ì„œ ì‚­ì œ í›„ ì¬ìƒì„±
kubectl -n data-plane-system delete certificate prod-scraping-run-wildcards
kubectl -n data-plane-system delete secret prod-scraping-run-wildcards-tls

# Certificate ë‹¤ì‹œ ìƒì„± (ìœ„ì˜ YAML ì¬ì ìš©)
```

---

## 7. Data Plane ë°°í¬

### 7.1 ë°°í¬ ì¤€ë¹„

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# ë°°í¬ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /path/to/data-plane/build

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export DOMAIN=prod.scraping.run
export NAMESPACE=data-plane-system
export DB_PV_SIZE=30Gi
export OSS_PV_SIZE=50Gi
export PROMETHEUS_PV_SIZE=20Gi
export EXTERNAL_HTTP_SCHEMA=https
export ENABLE_MONITOR=true

# ë„ë©”ì¸ í™•ì¸
host $DOMAIN
```

### 7.2 start.sh ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
# start.sh ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x start.sh

# Data Plane ì „ì²´ ë°°í¬ ì‹¤í–‰
./start.sh

# ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
# 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± (data-plane-system)
# 2. MongoDB 4.4 ë°°í¬ (mongodb-4.4.yaml ì‚¬ìš©)
# 3. Prometheus ì„¤ì¹˜ (ENABLE_MONITOR=trueì¸ ê²½ìš°)
# 4. MinIO ê°ì²´ ìŠ¤í† ë¦¬ì§€ ë°°í¬
# 5. Data Plane Server ë°°í¬
# 6. Data Plane Web ë°°í¬
# 7. ì‹œìŠ¤í…œ ê´€ë¦¬ì ê³„ì • ìƒì„±
```

### 7.3 ë°°í¬ ìƒíƒœ í™•ì¸

```bash
# ì „ì²´ Pod ìƒíƒœ í™•ì¸
kubectl -n data-plane-system get pods

# ì„œë¹„ìŠ¤ í™•ì¸
kubectl -n data-plane-system get svc

# Ingress í™•ì¸
kubectl -n data-plane-system get ingress

# PVC ìƒíƒœ í™•ì¸
kubectl -n data-plane-system get pvc
```

### 7.4 ë°°í¬ ê²€ì¦

```bash
# MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl -n data-plane-system exec mongodb-0 -- mongo --eval "db.version()"

# MinIO ìƒíƒœ í™•ì¸
kubectl -n data-plane-system logs -l app=minio --tail=20

# Data Plane Server ë¡œê·¸ í™•ì¸
kubectl -n data-plane-system logs -l app=data-plane-server --tail=50

# Web UI ì ‘ê·¼ í…ŒìŠ¤íŠ¸
curl -k https://$DOMAIN
curl -k https://api.$DOMAIN/healthz
```

### 7.5 ë¬¸ì œ í•´ê²°

#### Podê°€ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
# Pod ìƒì„¸ ì •ë³´ í™•ì¸
kubectl -n data-plane-system describe pod <POD_NAME>

# ì´ë²¤íŠ¸ í™•ì¸
kubectl -n data-plane-system get events --sort-by='.lastTimestamp'
```

#### PVCê°€ Pending ìƒíƒœì¸ ê²½ìš°
```bash
# StorageClass í™•ì¸
kubectl get storageclass

# OpenEBS ìƒíƒœ í™•ì¸
kubectl -n openebs get pods
```

#### Ingressê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
# Higress/Nginx Ingress Controller ìƒíƒœ í™•ì¸
kubectl -n higress-system get pods
kubectl -n ingress-nginx get pods

# Ingress ë¦¬ì†ŒìŠ¤ ìƒì„¸ ì •ë³´
kubectl -n data-plane-system describe ingress
```

### 7.6 ì´ˆê¸° ì„¤ì •

ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ í›„ ì¶œë ¥ë˜ëŠ” ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”:

```bash
# ì¶œë ¥ ì˜ˆì‹œ:
========================================
Data-Plane services:
========================================
API Server: https://api.prod.scraping.run
Web Console: https://prod.scraping.run
MinIO Console: https://minio.prod.scraping.run
OSS Endpoint: https://oss.prod.scraping.run

Admin credentials:
Username: admin
Password: [ìë™ ìƒì„±ëœ íŒ¨ìŠ¤ì›Œë“œ]

Please save these credentials securely!
========================================
```

---

## 8. ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜

### 8.1 í´ëŸ¬ìŠ¤í„° ìƒíƒœ ëª¨ë‹ˆí„°ë§

```bash
# ë…¸ë“œë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
kubectl top nodes

# Pod ë¶„í¬ í™•ì¸
kubectl get pods -A -o wide --field-selector spec.nodeName=instance-20250209-1502
kubectl get pods -A -o wide --field-selector spec.nodeName=instance-20250209-1504
kubectl get pods -A -o wide --field-selector spec.nodeName=instance-20250306-1735

# ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
kubectl -n data-plane-system get pods
kubectl -n data-plane-system get svc
kubectl -n data-plane-system get ingress

# MongoDB ìƒíƒœ í™•ì¸
kubectl exec -n data-plane-system mongodb-0 -- mongo --eval "rs.status()"
```

### 8.2 ì„œë¹„ìŠ¤ ì ‘ê·¼ í…ŒìŠ¤íŠ¸

```bash
# API ì„œë²„ í…ŒìŠ¤íŠ¸
curl -k https://api.prod.scraping.run/v1/regions

# Web UI ì ‘ê·¼
echo "Web UI: https://prod.scraping.run"
echo "API Server: https://api.prod.scraping.run"
echo "MinIO Console: https://minio.prod.scraping.run"
echo "OSS: https://oss.prod.scraping.run"
echo "Grafana: https://grafana.prod.scraping.run (if enabled)"
```

### 8.3 ë…¸ë“œ ì¥ì•  ë³µêµ¬

#### Worker ë…¸ë“œ ì¥ì•  ì‹œ
```bash
# 1. ì¥ì•  ë…¸ë“œ í™•ì¸
kubectl get nodes
kubectl describe node <failed-node>

# 2. ë…¸ë“œ ê²©ë¦¬ (ì„ íƒì )
kubectl cordon <failed-node>
kubectl drain <failed-node> --ignore-daemonsets --delete-emptydir-data

# 3. ë…¸ë“œ ë³µêµ¬ í›„
kubectl uncordon <failed-node>

# 4. Pod ì¬ë°°ì¹˜ í™•ì¸
kubectl get pods -A -o wide | grep <recovered-node>
```

#### Master ë…¸ë“œ ì¥ì•  ì‹œ (ë‹¨ì¼ ë§ˆìŠ¤í„°)
```bash
# 1. etcd ë°±ì—…ì—ì„œ ë³µêµ¬
sudo kubeadm reset
sudo kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml

# 2. ë„¤íŠ¸ì›Œí¬ í”ŒëŸ¬ê·¸ì¸ ì¬ì„¤ì¹˜
kubectl apply -f cilium.yaml

# 3. Worker ë…¸ë“œ ì¬ì¡°ì¸
# ê° ì›Œì»¤ ë…¸ë“œì—ì„œ:
sudo kubeadm reset
sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>
```

### 8.4 ë°±ì—… ë° ë³µêµ¬

#### etcd ë°±ì—…
```bash
# ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat <<'EOF' > /usr/local/bin/backup-etcd.sh
#!/bin/bash
BACKUP_DIR="/backup/etcd"
mkdir -p $BACKUP_DIR
ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save $BACKUP_DIR/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db
# 7ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
find $BACKUP_DIR -name "*.db" -mtime +7 -delete
EOF

chmod +x /usr/local/bin/backup-etcd.sh

# cron ë“±ë¡ (ë§¤ì¼ ìƒˆë²½ 2ì‹œ)
echo "0 2 * * * /usr/local/bin/backup-etcd.sh" | crontab -
```

#### MongoDB ë°±ì—…
```bash
# MongoDB ë°±ì—…
kubectl exec -n data-plane-system mongodb-0 -- mongodump --out=/tmp/backup
kubectl cp data-plane-system/mongodb-0:/tmp/backup ./mongodb-backup-$(date +%Y%m%d)

# MongoDB ë³µêµ¬
kubectl cp ./mongodb-backup data-plane-system/mongodb-0:/tmp/restore
kubectl exec -n data-plane-system mongodb-0 -- mongorestore /tmp/restore
```

### 8.5 ì„±ëŠ¥ íŠœë‹

#### ë…¸ë“œë³„ ë¦¬ì†ŒìŠ¤ ìµœì í™”
```bash
# instance-20250209-1502 (DB ë…¸ë“œ) - MongoDB ìš°ì„ 
kubectl label nodes instance-20250209-1502 node-type=database
kubectl taint nodes instance-20250209-1502 node-type=database:PreferNoSchedule

# instance-20250209-1504 (App ë…¸ë“œ) - ì• í”Œë¦¬ì¼€ì´ì…˜ ìš°ì„   
kubectl label nodes instance-20250209-1504 node-type=application

# instance-20250306-1735 (Storage ë…¸ë“œ) - ìŠ¤í† ë¦¬ì§€/ëª¨ë‹ˆí„°ë§ ìš°ì„ 
kubectl label nodes instance-20250306-1735 node-type=storage
```

#### Pod ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •
```yaml
# Data Plane Server ë¦¬ì†ŒìŠ¤ ìµœì í™”
resources:
  requests:
    memory: "2Gi"
    cpu: "500m"
  limits:
    memory: "4Gi"
    cpu: "2000m"
```

### 8.6 ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„

```bash
# ëª¨ë“  Pod ë¡œê·¸ í™•ì¸
kubectl logs -n data-plane-system -l app=data-plane-server --tail=100

# íŠ¹ì • ë…¸ë“œì˜ ì‹œìŠ¤í…œ ë¡œê·¸
ssh instance-20250209-1502 "sudo journalctl -u kubelet -f"

# ì»¨í…Œì´ë„ˆ ëŸ°íƒ€ì„ ë¡œê·¸
ssh instance-20250209-1502 "sudo journalctl -u containerd -f"

# Cilium ë„¤íŠ¸ì›Œí¬ ë””ë²„ê¹…
kubectl -n kube-system exec -it ds/cilium -- cilium status
kubectl -n kube-system exec -it ds/cilium -- cilium connectivity test
```

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1 ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

#### Podê°€ Pending ìƒíƒœì¼ ë•Œ
```bash
# ì›ì¸ í™•ì¸
kubectl describe pod <pod-name> -n <namespace>

# ë…¸ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl describe nodes | grep -A 5 "Allocated resources"

# PVC ìƒíƒœ í™•ì¸
kubectl get pvc -A
```

#### ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ
```bash
# Cilium ìƒíƒœ í™•ì¸
kubectl -n kube-system get pods -l k8s-app=cilium
cilium status

# DNS í™•ì¸
kubectl run test-dns --image=busybox --rm -it -- nslookup kubernetes.default

# Service ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run test-curl --image=curlimages/curl --rm -it -- curl http://data-plane-server.data-plane-system:9170
```

#### ì¸ì¦ì„œ ë¬¸ì œ
```bash
# ì¸ì¦ì„œ ìƒíƒœ í™•ì¸
kubectl get certificate -A
kubectl describe certificate -n data-plane-system

# cert-manager ë¡œê·¸ í™•ì¸
kubectl logs -n cert-manager deploy/cert-manager
```

### 9.2 4ë…¸ë“œ í´ëŸ¬ìŠ¤í„° íŠ¹ì • ì´ìŠˆ

#### ë…¸ë“œ ê°„ í†µì‹  ë¬¸ì œ
```bash
# Tailscale ì—°ê²° í™•ì¸
tailscale status

# ë…¸ë“œ ê°„ ping í…ŒìŠ¤íŠ¸
ping 100.64.0.2  # instance-20250209-1502
ping 100.64.0.3  # instance-20250209-1504
ping 100.64.0.4  # instance-20250306-1735

# iptables ê·œì¹™ í™•ì¸
sudo iptables -L -n | grep TAILSCALE
```

#### MongoDB ReplicaSet ë¬¸ì œ
```bash
# Primary ë…¸ë“œ í™•ì¸
kubectl exec -n data-plane-system mongodb-0 -- mongo --eval "rs.isMaster()"

# ê°•ì œ Primary ì„ ì¶œ
kubectl exec -n data-plane-system mongodb-0 -- mongo --eval "rs.stepDown()"

# ReplicaSet ì¬êµ¬ì„±
kubectl exec -n data-plane-system mongodb-0 -- mongo --eval "rs.reconfig(rs.conf())"
```

#### ë…¸ë“œë³„ Pod ë¶„í¬ ë¶ˆê· í˜•
```bash
# Pod ë¶„í¬ ì¬ì¡°ì •
kubectl rollout restart deployment/data-plane-server -n data-plane-system
kubectl rollout restart deployment/data-plane-web -n data-plane-system

# ìˆ˜ë™ Pod ì´ë™
kubectl delete pod <pod-name> -n <namespace>
```

---

## 10. ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 10.1 ìë™í™” ìŠ¤í¬ë¦½íŠ¸

ì „ì²´ ì„¤ì¹˜ë¥¼ ìë™í™”í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸:

```bash
#!/bin/bash
# quick-setup.sh - 4ë…¸ë“œ HA í´ëŸ¬ìŠ¤í„° ë¹ ë¥¸ ì„¤ì¹˜

set -e

echo "==================================="
echo "4-Node HA Kubernetes Quick Setup"
echo "==================================="

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export DOMAIN=${DOMAIN:-prod.scraping.run}
export NAMESPACE=${NAMESPACE:-data-plane-system}
export NODE_TYPE=${1:-master}  # master, instance-20250209-1502, instance-20250209-1504, instance-20250306-1735

case $NODE_TYPE in
  master)
    echo "Setting up Master node (instance-20250216-2117)..."
    # 1. Kubernetes ì´ˆê¸°í™”
    sudo kubeadm init --pod-network-cidr=10.0.0.0/8 \
      --apiserver-advertise-address=100.64.0.1
    
    # 2. kubeconfig ì„¤ì •
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
    
    # 3. Cilium ì„¤ì¹˜
    helm install cilium cilium/cilium --version 1.17.5 \
      --namespace kube-system \
      --set operator.replicas=1
    
    # 4. Join í† í° ìƒì„±
    echo "Join command for workers:"
    kubeadm token create --print-join-command > /tmp/join-command.txt
    cat /tmp/join-command.txt
    ;;
    
  worker*)
    echo "Setting up Worker node ($NODE_TYPE)..."
    echo "Please run the join command from master node"
    ;;
esac

echo "âœ… Node setup completed!"
```

### 10.2 ì„¤ì¹˜ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# ì²´í¬ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
cat <<'EOF' > verify-installation.sh
#!/bin/bash
echo "=== 4-Node HA Cluster Verification ==="
echo ""

# 1. ë…¸ë“œ ìƒíƒœ
echo "âœ“ Checking nodes..."
kubectl get nodes

# 2. ì‹œìŠ¤í…œ Pod
echo "âœ“ Checking system pods..."
kubectl get pods -n kube-system

# 3. Data Plane ì„œë¹„ìŠ¤
echo "âœ“ Checking Data Plane services..."
kubectl get all -n data-plane-system

# 4. ì¸ê·¸ë ˆìŠ¤
echo "âœ“ Checking ingress..."
kubectl get ingress -A

# 5. ìŠ¤í† ë¦¬ì§€
echo "âœ“ Checking storage..."
kubectl get pv,pvc -A

# 6. ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸
echo "âœ“ Service endpoints:"
echo "  - Web: https://$DOMAIN"
echo "  - API: https://api.$DOMAIN"
echo "  - MinIO: https://minio.$DOMAIN"
echo "  - OSS: https://oss.$DOMAIN"

echo ""
echo "=== Verification Complete ==="
EOF

chmod +x verify-installation.sh
./verify-installation.sh
```

---

## ë¶€ë¡ A: ì°¸ê³  ëª…ë ¹ì–´

### ìœ ìš©í•œ kubectl ëª…ë ¹ì–´
```bash
# ë…¸ë“œë³„ Pod ìˆ˜ í™•ì¸
kubectl get pods -A -o json | jq '.items | group_by(.spec.nodeName) | map({node: .[0].spec.nodeName, count: length})'

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìƒìœ„ Pod
kubectl top pods -A --sort-by=memory | head -20

# ìµœê·¼ ì´ë²¤íŠ¸
kubectl get events -A --sort-by='.lastTimestamp' | tail -20

# Pod ì¬ì‹œì‘
kubectl rollout restart deployment/<name> -n <namespace>

# ê°•ì œ ì‚­ì œ
kubectl delete pod <pod> -n <namespace> --force --grace-period=0
```

### ë””ë²„ê¹… ëª…ë ¹ì–´
```bash
# ì„ì‹œ ë””ë²„ê·¸ Pod ì‹¤í–‰
kubectl run debug --image=nicolaka/netshoot --rm -it -- /bin/bash

# íŠ¹ì • ë…¸ë“œì—ì„œ Pod ì‹¤í–‰
kubectl run debug --image=busybox --rm -it --overrides='{"spec":{"nodeSelector":{"kubernetes.io/hostname":"instance-20250209-1502"}}}' -- /bin/sh

# ë„¤íŠ¸ì›Œí¬ ì •ì±… í™•ì¸
kubectl get networkpolicies -A
```

---

## ë¶€ë¡ B: ì„¤ì • íŒŒì¼ í…œí”Œë¦¿

### values-override.yaml (HA ì„¤ì •)
```yaml
# Data Plane HA ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
replicaCount: 3

affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - data-plane-server
      topologyKey: kubernetes.io/hostname

resources:
  requests:
    memory: "2Gi"
    cpu: "1"
  limits:
    memory: "4Gi"
    cpu: "2"

persistence:
  enabled: true
  storageClass: openebs-hostpath
  size: 30Gi
```

---

ì´ ë¬¸ì„œëŠ” Oracle Cloud í™˜ê²½ì—ì„œ 4ë…¸ë“œ ê³ ê°€ìš©ì„± Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ìš”ì•½

- **í´ëŸ¬ìŠ¤í„° êµ¬ì„±**: 1 Master + 3 Worker (4ë…¸ë“œ HA)
- **ë„¤íŠ¸ì›Œí‚¹**: Tailscale VPN + Cilium CNI
- **ìŠ¤í† ë¦¬ì§€**: OpenEBS LocalPV
- **Ingress**: Higress v2.1.6 (nginx í˜¸í™˜)
- **ë°ì´í„°ë² ì´ìŠ¤**: MongoDB 4.4 (ARM64 í˜¸í™˜)
- **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana (ì„ íƒì )

ê° ë…¸ë“œë³„ ì—­í• :
- **instance-20250216-2117**: Control Plane ì „ìš©
- **instance-20250209-1502**: MongoDB Primary, Data Plane Server
- **instance-20250209-1504**: MongoDB Secondary, Data Plane Server, Web
- **instance-20250306-1735**: MinIO, ëª¨ë‹ˆí„°ë§, ë°±ì—…

ë¬¸ì œ ë°œìƒ ì‹œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜ì„ ì°¸ì¡°í•˜ê±°ë‚˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.